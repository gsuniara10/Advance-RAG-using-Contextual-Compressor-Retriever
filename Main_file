from langchain_ollama import OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma 
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

load_dotenv()

loading_document = PyPDFLoader("Machine Learning. An Algorithmic Perspective 2nd ed.pdf")

document_load = loading_document.load()

embedding_llm = HuggingFaceEmbeddings(
    model_name= "sentence-transformers/all-MiniLM-L6-v2"
)

simple_llm  = ChatGroq(
     model="llama-3.1-8b-instant"
)

# embedding_llm = OllamaEmbeddings(
#     model="llama3"
# )

splitting_document = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50 )

document_split = splitting_document.split_documents(document_load)

storing_vectore_docs = Chroma.from_documents(
    documents=document_split,
    embedding=embedding_llm,
    collection_name="my_collection"
    )

retriever= storing_vectore_docs.as_retriever(search_kwargs={"k":4})

compressor = LLMChainExtractor.from_llm(llm=simple_llm)

compressor_retriever = ContextualCompressionRetriever(
    base_retriever=retriever,
    base_compressor= compressor
)


query = input("Enter the query : ")


compressor_output = compressor_retriever.invoke(query)

print(compressor_output)
